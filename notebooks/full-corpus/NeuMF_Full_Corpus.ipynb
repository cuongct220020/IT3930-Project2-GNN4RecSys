{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17620,"status":"ok","timestamp":1749834616392,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"},"user_tz":-420},"id":"yzODmV8kexi5","outputId":"27041f5d-046f-4fae-a004-5540b56fcb13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Nội dung trong thư mục:\n","['results', 'preprocess', 'data', 'models', 'other']\n"]}],"source":["from google.colab import drive\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Yêu cầu quyền truy cập vào Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Đường dẫn thư mục project\n","project_dir = '/content/drive/My Drive/Project II/'\n","\n","# Kiểm tra xem project_dir có tồn tại không trước khi thay đổi thư mục làm việc\n","if os.path.exists(project_dir):\n","    os.chdir(project_dir)\n","    print(\"Nội dung trong thư mục:\")\n","    print(os.listdir())  # Liệt kê nội dung thư mục\n","else:\n","    print(f\"Thư mục không tồn tại: {project_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"P2JdHC4kKfC7"},"source":["# **1. Khai báo thư viện**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNHzT7vVQjwO"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import Tensor\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.nn.modules.loss import _Loss\n","\n","import scipy.sparse as sp\n","import random\n","from scipy.special import expit\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mofpjMh-Z-UD"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# === NeuMF ===\n","model_dir = '/content/drive/My Drive/Project II/models/ckpt/neumf_full_corpus.pth'"]},{"cell_type":"markdown","metadata":{"id":"6lMJm0tiKiad"},"source":["# **2. Load và chia dữ liệu**"]},{"cell_type":"markdown","metadata":{"id":"-ITOBbhJad7e"},"source":["## **2.1. Chia dữ liệu theo chiến lược Full-Corpus**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fGvUrhfmqbx"},"outputs":[],"source":["# Load dữ liệu\n","data_dir = project_dir + \"data/\"\n","dataset = pd.read_csv(data_dir + \"recommendations_processed.csv\").copy()  # Thêm .copy() để tạo bản sao rõ ràng\n","\n","# Explicitly convert 'date' column to datetime objects\n","dataset['date'] = pd.to_datetime(dataset['date'])\n","\n","# Loại trừ duplicate\n","dataset = dataset.sort_values(\"date\")  # Sort theo thời gian\n","dataset = dataset.drop_duplicates(subset=['user_id', 'app_id'], keep='last')\n","\n","# Mapping user_id, game_id sang user_index, game_index\n","all_user_ids = dataset['user_id'].unique()\n","all_game_ids = dataset['app_id'].unique()\n","\n","user_id_mapping = {user_id: idx for idx, user_id in enumerate(sorted(all_user_ids))}\n","game_id_mapping = {item_id: idx for idx, item_id in enumerate(sorted(all_game_ids))}\n","\n","# Cập nhật lại user_id, app_id sử dụng .loc\n","dataset.loc[:, 'user_id'] = dataset['user_id'].map(user_id_mapping)\n","dataset.loc[:, 'app_id'] = dataset['app_id'].map(game_id_mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":692,"status":"ok","timestamp":1749834728394,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"},"user_tz":-420},"id":"o_vAhjszmXGk","outputId":"4d8d2e8f-9736-4a63-d2ba-b6f8fa8c58d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_df: (1571114, 8)\n","valid_df: (486039, 8)\n","test_df: (477880, 8)\n"]}],"source":["def split_data_full_corpus(df, ratio=0.918):\n","    # Với tỉ lệ thời gian 0.918 thì sẽ ra được tỉ lệ số lượng ~80%\n","    df = df.sort_values(\"date\")\n","    start_date = df[\"date\"].min()\n","    end_date = df[\"date\"].max()\n","    pivot_date = start_date + (end_date - start_date) * ratio\n","    train_set = df[df[\"date\"] < pivot_date].copy()\n","    test_set = df[df[\"date\"] >= pivot_date].copy()\n","    return train_set, test_set\n","\n","# Chia dữ liệu theo chiến lược full corpus\n","train_df, test_df = split_data_full_corpus(dataset)\n","train_df, valid_df = split_data_full_corpus(train_df)\n","\n","print(f\"train_df: {train_df.shape}\")\n","print(f\"valid_df: {valid_df.shape}\")\n","print(f\"test_df: {test_df.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"6f6sWCGyaSLL"},"source":["## **2.2. Thống kê đơn giản**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1749834728923,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"},"user_tz":-420},"id":"yFXq83HqegXR","outputId":"c28f1977-13f6-46fb-f9e2-c89a37111e8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Số lượng users: 47274\n","Số lượng games: 4632\n","Số lượng nodes: 51906\n","Full-Corpus Evaluation\n","\n","=== Cold-start analysis in valid ===\n","Total users in valid: 45039\n","--> Cold-start users in valid: 903\n","Total items in valid: 4121\n","--> Cold-start games in valid: 524\n","\n","=== Cold-start analysis in test ===\n","Total users in test: 47274\n","--> Cold-start users in test: 903\n","Total items in test: 4609\n","--> Cold-start games in test: 1029\n"]}],"source":["# Số lượng users, games\n","num_users = len(all_user_ids)\n","num_games = len(all_game_ids)\n","num_nodes = num_users + num_games\n","\n","print(f\"Số lượng users: {num_users}\")\n","print(f\"Số lượng games: {num_games}\")\n","print(f\"Số lượng nodes: {num_nodes}\")\n","\n","# Print cold-start statistics\n","train_users = set(train_df['user_id'].unique())\n","train_items = set(train_df['app_id'].unique())\n","\n","valid_users = set(valid_df['user_id'].unique())\n","valid_items = set(valid_df['app_id'].unique())\n","\n","test_users = set(test_df['user_id'].unique())\n","test_items = set(test_df['app_id'].unique())\n","\n","valid_cold_start_users = valid_users - train_users\n","valid_cold_start_items = valid_items - train_items\n","test_cold_start_users = test_users - train_users\n","test_cold_start_items = test_items - train_items\n","\n","print(\"Full-Corpus Evaluation\")\n","print(\"\\n=== Cold-start analysis in valid ===\")\n","print(f\"Total users in valid: {len(valid_users)}\")\n","print(f\"--> Cold-start users in valid: {len(valid_cold_start_users)}\")\n","print(f\"Total items in valid: {len(valid_items)}\")\n","print(f\"--> Cold-start games in valid: {len(valid_cold_start_items)}\")\n","\n","print(\"\\n=== Cold-start analysis in test ===\")\n","print(f\"Total users in test: {len(test_users)}\")\n","print(f\"--> Cold-start users in test: {len(test_cold_start_users)}\")\n","print(f\"Total items in test: {len(test_items)}\")\n","print(f\"--> Cold-start games in test: {len(test_cold_start_items)}\")"]},{"cell_type":"markdown","metadata":{"id":"uWRNpO7ZRy2B"},"source":["# **3. Các hàm tiện ích**"]},{"cell_type":"markdown","source":["## **3.1. Các lớp bao dữ liệu**"],"metadata":{"id":"FX9RdSxsH_t9"}},{"cell_type":"code","source":["class Data:\n","    def __init__(self, pos_users, pos_items, all_neg_items, user_to_idx):\n","        self.pos_users = pos_users\n","        self.pos_items = pos_items\n","        self.all_neg_items = all_neg_items\n","        self.user_to_idx = user_to_idx\n","\n","class FullCorpusData:\n","    def __init__(self, labels, has_interacted_masks, valid_users, warm_item_mask):\n","        self.labels = labels  # Ground truth labels [num_users, num_games]\n","        self.has_interacted_masks = has_interacted_masks  # Boolean mask of interactions [num_users, num_games]\n","        self.valid_users = valid_users  # Tensor of user IDs\n","        self.warm_item_mask = warm_item_mask  # Boolean mask of warm items [num_games]\n","\n","class TestData:\n","    def __init__(self, labels: torch.Tensor, has_interacted_masks: torch.Tensor,\n","                 user_ids: torch.Tensor, warm_items_mask: torch.Tensor,\n","                 warm_users_mask: torch.Tensor):\n","        self.labels = labels  # Ground truth labels [num_users, num_games]\n","        self.has_interacted_masks = has_interacted_masks  # Boolean mask of interactions [num_users, num_games]\n","        self.user_ids = user_ids  # Tensor of user IDs\n","        self.warm_items_mask = warm_items_mask  # Boolean mask of warm items [num_games]\n","        self.warm_users_mask = warm_users_mask  # Boolean mask of warm users [num_users]"],"metadata":{"id":"kwIkSmkcIJjb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3.2. Hàm tạo ma trận tương tác user-item**"],"metadata":{"id":"L0BXNz7gIEiT"}},{"cell_type":"code","source":["def create_interact_matrix(dataset: pd.DataFrame, num_users: int, num_items: int) -> torch.BoolTensor:\n","    # Chỉ lấy các tương tác positive trên toàn bộ dataset\n","    pos_df = dataset[dataset['is_recommended'] == 1]\n","    rows = pos_df['user_id'].to_numpy()\n","    cols = pos_df['app_id'].to_numpy()\n","    data = np.ones_like(rows, dtype=np.bool_)\n","    mat = sp.coo_matrix((data, (rows, cols)), shape=(num_users, num_items))\n","    return torch.from_numpy(mat.toarray())  # dtype=bool mặc định\n","\n","full_matrix = create_interact_matrix(dataset, num_users, num_games)\n","train_matrix = create_interact_matrix(train_df, num_users, num_games)\n","valid_matrix = create_interact_matrix(valid_df, num_users, num_games)"],"metadata":{"id":"R42uRGHzIJ8b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uXVi6f6iVgBG"},"source":["## **3.3. Hàm định nghĩa các Ranking Metrics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtMsULMbK3VB"},"outputs":[],"source":["def precision_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","    return hits.sum(dim=1) / k\n","\n","def recall_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","    relevant = labels.sum(dim=1).clamp(min=1e-8)  # avoid divide by zero\n","    return hits.sum(dim=1) / relevant\n","\n","def ndcg_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    device = scores.device\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","\n","    weights = torch.log2(torch.arange(2, k + 2, device=device).float())\n","    dcg = (hits / weights).sum(dim=1)\n","\n","    ideal_len = labels.sum(dim=1).clamp(max=k).long()\n","    idcg = torch.stack([\n","        (1.0 / weights[:L]).sum() if L > 0 else torch.tensor(0.0, device=device)\n","        for L in ideal_len\n","    ])\n","    return dcg / idcg.clamp(min=1e-8)\n","\n","def hitrate_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","    return (hits.sum(dim=1) > 0).float()"]},{"cell_type":"markdown","source":["## **3.4. Hàm mất mát BPR**"],"metadata":{"id":"Tz9dWGoOLWpq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-LZ40WtNC5X"},"outputs":[],"source":["class BPRLoss(_Loss):\n","    def __init__(self, lambda_reg: float = 1e-3):\n","        super().__init__()\n","        self.lambda_reg = lambda_reg\n","\n","    def forward(self,\n","                pos_score: Tensor,\n","                neg_score: Tensor,\n","                parameters: Tensor = None) -> Tensor:\n","        \"\"\"\n","        pos_score: [batch_size]\n","        neg_score: [batch_size, num_neg]\n","        parameters: embedding cần regularize\n","        \"\"\"\n","        # Ensure pos_score has shape [batch_size, 1] for broadcasting\n","        if pos_score.dim() == 1:\n","            pos_score = pos_score.unsqueeze(1)  # [batch_size, 1]\n","\n","        log_prob = F.logsigmoid(pos_score - neg_score).mean()\n","\n","        regularization = 0\n","        if self.lambda_reg != 0 and parameters is not None:\n","            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n","            regularization = regularization / pos_score.size(0)\n","\n","        return -log_prob + regularization"]},{"cell_type":"markdown","metadata":{"id":"hOUxGD-NPsmj"},"source":["# **4. Định nghĩa mô hình NeuMF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_7tZFGWQDil"},"outputs":[],"source":["class NeuMF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_dim, layers, dropout):\n","        super(NeuMF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","\n","        # GMF embeddings\n","        self.user_emb_gmf = nn.Embedding(num_users, embedding_dim)\n","        self.item_emb_gmf = nn.Embedding(num_items, embedding_dim)\n","\n","        # MLP embeddings\n","        self.user_emb_mlp = nn.Embedding(num_users, layers[0] // 2)\n","        self.item_emb_mlp = nn.Embedding(num_items, layers[0] // 2)\n","\n","        # MLP layers\n","        mlp = []\n","        for i in range(len(layers) - 1):\n","            mlp.append(nn.Linear(layers[i], layers[i+1]))\n","            mlp.append(nn.ReLU())\n","            if dropout:\n","                mlp.append(nn.Dropout(dropout))\n","        self.mlp = nn.Sequential(*mlp)\n","\n","        # Prediction layer\n","        self.head = nn.Linear(layers[-1] + embedding_dim, 1)\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        # Initialize embeddings\n","        nn.init.normal_(self.user_emb_gmf.weight, std=0.01)\n","        nn.init.normal_(self.item_emb_gmf.weight, std=0.01)\n","        nn.init.normal_(self.user_emb_mlp.weight, std=0.01)\n","        nn.init.normal_(self.item_emb_mlp.weight, std=0.01)\n","\n","        # Initialize MLP layers\n","        for layer in self.mlp:\n","            if isinstance(layer, nn.Linear):\n","                nn.init.xavier_uniform_(layer.weight)\n","                if layer.bias is not None:\n","                    nn.init.zeros_(layer.bias)\n","\n","        # Initialize prediction layer\n","        nn.init.xavier_uniform_(self.head.weight)\n","        nn.init.zeros_(self.head.bias)\n","\n","    def forward(self, user_idx, item_idx):\n","        # GMF path\n","        user_emb_gmf = self.user_emb_gmf(user_idx)  # [batch_size, embedding_dim]\n","        item_emb_gmf = self.item_emb_gmf(item_idx)  # [batch_size, embedding_dim]\n","        gmf_vector = user_emb_gmf * item_emb_gmf    # [batch_size, embedding_dim]\n","\n","        # MLP path\n","        user_emb_mlp = self.user_emb_mlp(user_idx)  # [batch_size, layers[0]//2]\n","        item_emb_mlp = self.item_emb_mlp(item_idx)  # [batch_size, layers[0]//2]\n","        mlp_vector = torch.cat([user_emb_mlp, item_emb_mlp], dim=1)  # [batch_size, layers[0]]\n","        mlp_vector = self.mlp(mlp_vector)  # [batch_size, layers[-1]]\n","\n","        # Combine paths\n","        vector = torch.cat([gmf_vector, mlp_vector], dim=1)  # [batch_size, layers[-1] + embedding_dim]\n","        scores = self.head(vector).squeeze(-1)  # [batch_size]\n","        return scores\n","\n","    def compute_bpr_loss(self, pos_scores: torch.Tensor,\n","                         neg_scores: torch.Tensor,\n","                         lambda_reg: float = 1e-3) -> torch.Tensor:\n","        loss_fn = BPRLoss(lambda_reg=lambda_reg)\n","\n","        all_params = []\n","        for param in self.parameters():\n","            if param.requires_grad:\n","                all_params.append(param.view(-1))\n","\n","        concatenated_params = torch.cat(all_params) if all_params else None\n","        return loss_fn(pos_scores, neg_scores, concatenated_params)"]},{"cell_type":"markdown","metadata":{"id":"qeS_iT0RWWOu"},"source":["# **5. Huấn luyện mô hình**"]},{"cell_type":"markdown","metadata":{"id":"ry0gMsf4PsD-"},"source":["## **5.1. Định nghĩa hàm train**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_cZFDYoHp8Q"},"outputs":[],"source":["def train(model, train_data, train_loader, scheduler, optimizer, device, lambda_reg=1e-3, criterion='bpr', num_neg=3):\n","    model.train()\n","    total_loss = 0.0\n","    total_examples = 0\n","\n","    num_users = model.num_users\n","    num_items = model.num_items\n","\n","    pos_users = train_data.pos_users.to(device)\n","    pos_items = train_data.pos_items.to(device)\n","    all_neg_items = train_data.all_neg_items.to(device)\n","    user_to_train_idx = train_data.user_to_idx\n","\n","    for batch_index in tqdm(train_loader, desc=\"Training\", leave=False):\n","        # === Positive Samples ===\n","        batch_pos_users = pos_users[batch_index]\n","        batch_pos_items = pos_items[batch_index]\n","        batch_size = batch_pos_users.size(0)\n","\n","        # === Negative Sampling ===\n","        batch_train_indices = torch.tensor(\n","            [user_to_train_idx[u.item()] for u in batch_pos_users],\n","            dtype=torch.long, device=device\n","        )\n","        user_neg_items = all_neg_items[batch_train_indices]  # [batch_size, num_items]\n","        neg_items = torch.multinomial(user_neg_items.float(), num_samples=num_neg, replacement=True)  # [batch_size, num_neg]\n","\n","        # Expand users to match negative samples\n","        batch_neg_users = batch_pos_users.unsqueeze(1).expand(-1, num_neg).reshape(-1)  # [batch_size * num_neg]\n","        batch_neg_items = neg_items.reshape(-1)  # [batch_size * num_neg]\n","\n","        # === Compute scores ===\n","        pos_scores = model(batch_pos_users, batch_pos_items)  # [batch_size]  # Use batch_pos_items instead of batch_neg_items\n","        neg_scores = model(batch_neg_users, batch_neg_items)  # [batch_size * num_neg]\n","        neg_scores = neg_scores.view(batch_size, num_neg)  # [batch_size, num_neg]\n","\n","        optimizer.zero_grad()\n","        if criterion == 'bpr':\n","            loss = model.compute_bpr_loss(pos_scores=pos_scores,\n","                                        neg_scores=neg_scores,\n","                                        lambda_reg=lambda_reg)\n","        else:\n","            raise ValueError(f\"Unknown Loss Type: {criterion}\")\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * batch_size\n","        total_examples += batch_size\n","\n","    scheduler.step()\n","    train_loss = total_loss / total_examples if total_examples > 0 else 0.0\n","    return train_loss"]},{"cell_type":"markdown","metadata":{"id":"JFY9iRC2gOh_"},"source":["## **5.2. Định nghĩa hàm validate**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGxh6gL3Ip_r"},"outputs":[],"source":["@torch.no_grad()\n","def validate(model, valid_data, valid_loader, device, criterion='bpr', num_neg=3):\n","    model.eval()\n","    total_loss = 0.0\n","    total_examples = 0\n","\n","    num_users = model.num_users\n","    num_items = model.num_items\n","\n","    pos_users = valid_data.pos_users.to(device)\n","    pos_items = valid_data.pos_items.to(device)\n","    all_neg_items = valid_data.all_neg_items.to(device)\n","    user_to_valid_idx = valid_data.user_to_idx\n","\n","    for batch_index in tqdm(valid_loader, desc=\"Validating\", leave=False):\n","        # === Positive Samples ===\n","        batch_pos_users = pos_users[batch_index]\n","        batch_pos_items = pos_items[batch_index]\n","        batch_size = batch_pos_users.size(0)\n","\n","        # === Negative Sampling ===\n","        batch_valid_indices = torch.tensor(\n","            [user_to_valid_idx[u.item()] for u in batch_pos_users],\n","            dtype=torch.long, device=device\n","        )\n","        user_neg_items = all_neg_items[batch_valid_indices]  # [batch_size, num_items]\n","        neg_items = torch.multinomial(user_neg_items.float(), num_samples=num_neg, replacement=True)  # [batch_size, num_neg]\n","\n","        # Expand users to match negative samples\n","        batch_neg_users = batch_pos_users.unsqueeze(1).expand(-1, num_neg).reshape(-1)  # [batch_size * num_neg]\n","        batch_neg_items = neg_items.reshape(-1)  # [batch_size * num_neg]\n","\n","        # === Compute scores ===\n","        pos_scores = model(batch_pos_users, batch_pos_items)  # [batch_size]  # Use batch_pos_items instead of batch_neg_items\n","        neg_scores = model(batch_neg_users, batch_neg_items)  # [batch_size * num_neg]\n","        neg_scores = neg_scores.view(batch_size, num_neg)  # [batch_size, num_neg]\n","\n","        # === Compute loss ===\n","        loss = model.compute_bpr_loss(pos_scores=pos_scores,\n","                                    neg_scores=neg_scores,\n","                                    lambda_reg=0.0)\n","\n","        total_loss += loss.item() * batch_size\n","        total_examples += batch_size\n","\n","    valid_loss = total_loss / total_examples if total_examples > 0 else 0.0\n","    return valid_loss"]},{"cell_type":"markdown","metadata":{"id":"RIsvzswjUbcg"},"source":["## **5.3. Định nghĩa hàm evaluate**"]},{"cell_type":"code","source":["@torch.no_grad()\n","def evaluate_full_corpus(model, full_corpus_data, device, k=10, batch_size=512, item_batch_size=500):\n","    model.eval()\n","\n","    valid_labels = full_corpus_data.labels.to(device)\n","    has_interacted_masks = full_corpus_data.has_interacted_masks.to(device)\n","    users_global = full_corpus_data.valid_users.to(device)\n","    warm_item_mask = full_corpus_data.warm_item_mask.to(device)\n","\n","    # Filter for warm-start items\n","    valid_labels = valid_labels[:, warm_item_mask]\n","    has_interacted_masks = has_interacted_masks[:, warm_item_mask]\n","\n","    all_ndcg = []\n","    all_hit = []\n","\n","    num_valid = users_global.size(0)\n","    num_items = warm_item_mask.sum().item()  # Use number of warm items instead of all items\n","\n","    for start in tqdm(range(0, num_valid, batch_size), desc=\"Evaluating Full-Corpus\", leave=False):\n","        end = min(start + batch_size, num_valid)\n","        batch_users = users_global[start:end]\n","\n","        # Initialize scores tensor for this batch of users\n","        batch_scores = torch.full((batch_users.size(0), num_items), -float('inf'), device=device)\n","\n","        # Process items in chunks\n","        for item_start in range(0, num_items, item_batch_size):\n","            item_end = min(item_start + item_batch_size, num_items)\n","\n","            # Create user-item pairs for this chunk\n","            batch_users_expanded = batch_users.unsqueeze(1).expand(-1, item_end - item_start)\n","            chunk_items = torch.arange(item_start, item_end, device=device).unsqueeze(0).expand(batch_users.size(0), -1)\n","\n","            # Reshape for model forward pass\n","            batch_users_flat = batch_users_expanded.reshape(-1)\n","            chunk_items_flat = chunk_items.reshape(-1)\n","\n","            # Get scores for this chunk\n","            chunk_scores = model(batch_users_flat, chunk_items_flat)\n","\n","            # Reshape scores back to original dimensions\n","            chunk_scores = chunk_scores.reshape(batch_users.size(0), item_end - item_start)\n","\n","            # Store scores in the appropriate slice\n","            batch_scores[:, item_start:item_end] = chunk_scores\n","\n","            # Clear memory\n","            del chunk_scores\n","            torch.cuda.empty_cache()\n","\n","        # Mask out items user has already interacted with\n","        batch_scores = batch_scores.masked_fill(has_interacted_masks[start:end], -float('inf'))\n","\n","        # Get labels for this batch\n","        labels = valid_labels[start:end]\n","\n","        # Calculate metrics\n","        ndcg = ndcg_at_k(batch_scores, labels, k)\n","        hitrate = hitrate_at_k(batch_scores, labels, k)\n","\n","        all_ndcg.append(ndcg)\n","        all_hit.append(hitrate)\n","\n","        # Clear memory\n","        del batch_scores\n","        torch.cuda.empty_cache()\n","\n","    avg_ndcg = torch.cat(all_ndcg).mean().item() if all_ndcg else 0.0\n","    avg_hitrate = torch.cat(all_hit).mean().item() if all_hit else 0.0\n","    return avg_ndcg, avg_hitrate"],"metadata":{"id":"Znraz0PDKgc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSyh47F5P8tm"},"source":["## **5.4. Train Loss, Valid Loss, NDCG@10, HitRate@10**"]},{"cell_type":"markdown","metadata":{"id":"r9yOlVVrYe9d"},"source":["### **5.3.1. Chuẩn bị dữ liệu trước khi huấn luyện**"]},{"cell_type":"code","source":["# Create mapping from global user indices to training user indices\n","train_users_tensor = torch.tensor(list(train_users), dtype=torch.long)\n","train_num_users = train_users_tensor.size(0)\n","user_to_train_idx = {u.item(): i for i, u in enumerate(train_users_tensor)}\n","\n","# Extract positive interactions from train_df\n","train_pos_df = train_df[train_df['is_recommended'] == 1]\n","train_pos_users = torch.tensor(train_pos_df['user_id'].values, dtype=torch.long)\n","train_pos_items = torch.tensor(train_pos_df['app_id'].values, dtype=torch.long)\n","\n","# Pre-compute all possible negative items for each user in train dataset\n","train_all_neg_items = torch.zeros((train_num_users, num_games), dtype=torch.bool)\n","for i, u in enumerate(train_users_tensor):\n","    train_all_neg_items[i] = ~full_matrix[u]\n","\n","# Create Data object\n","train_data = Data(\n","    pos_users=train_pos_users,\n","    pos_items=train_pos_items,\n","    all_neg_items=train_all_neg_items,\n","    user_to_idx=user_to_train_idx\n",")"],"metadata":{"id":"EoR1vZdfPdhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_users_in_matrix = torch.tensor(list(valid_users), dtype=torch.long)\n","valid_num_users = valid_users_in_matrix.size(0)\n","user_to_valid_idx = {u.item(): i for i, u in enumerate(valid_users_in_matrix)}\n","\n","# Extract positive interactions from valid_df\n","valid_pos_df = valid_df[valid_df['is_recommended'] == 1]\n","valid_pos_users = torch.tensor(valid_pos_df['user_id'].values, dtype=torch.long)\n","valid_pos_items = torch.tensor(valid_pos_df['app_id'].values, dtype=torch.long)\n","\n","# Pre-compute negative items for validation dataset\n","valid_all_neg_items = torch.zeros((valid_num_users, num_games), dtype=torch.bool)\n","# Iterate through the mapped user IDs present in the validation matrix\n","for i, mapped_user_id in enumerate(valid_users_in_matrix):\n","    # Use the mapped_user_id to index the full_matrix (or valid_matrix if preferred, but full_matrix is consistent with train)\n","    valid_all_neg_items[i] = ~full_matrix[mapped_user_id]\n","\n","# Create Data object\n","valid_data = Data(\n","    pos_users=valid_pos_users,\n","    pos_items=valid_pos_items,\n","    all_neg_items=valid_all_neg_items,\n","    user_to_idx=user_to_valid_idx\n",")"],"metadata":{"id":"7_ZPUfmSPd1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create data loader\n","train_loader = DataLoader(\n","    dataset = range(len(train_data.pos_users)),\n","    batch_size = 8192,\n","    shuffle = True,\n","    num_workers = 2,\n","    pin_memory=True\n",")\n","\n","# Create data loader\n","valid_loader = DataLoader(\n","    dataset=range(len(valid_data.pos_users)),\n","    batch_size=8192,\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")"],"metadata":{"id":"GpZAh9HcPkrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCgYLjfMLuDU"},"outputs":[],"source":["def create_full_corpus_data(dataset_df, train_df, valid_df, num_games, device, min_neg=9):\n","    # Build game_list from the entire dataset\n","    game_list = sorted(dataset_df[\"app_id\"].unique())\n","    game_to_idx = {game_id: idx for idx, game_id in enumerate(game_list)}\n","    num_games_filtered = len(game_list)  # Should now be equal to num_games\n","\n","    # Danh sách user trong validation\n","    valid_user_ids = sorted(set(valid_df[\"user_id\"]))\n","    user_to_idx = {user_id: idx for idx, user_id in enumerate(valid_user_ids)}\n","    num_valid_users = len(valid_user_ids)\n","\n","    # Tạo cấu trúc dữ liệu cho user_played_games (từ train set)\n","    user_played_games = defaultdict(set)\n","    for user_id, game_id in zip(train_df[\"user_id\"], train_df[\"app_id\"]):\n","        user_played_games[user_id].add(game_id)\n","\n","    # Tạo cấu trúc dữ liệu cho user_valid (từ valid set)\n","    user_valid = defaultdict(list)\n","    for user_id, game_id, label in zip(valid_df[\"user_id\"], valid_df[\"app_id\"], valid_df[\"is_recommended\"]):\n","        user_valid[user_id].append((game_id, label))\n","\n","    # Khởi tạo valid_labels và has_interacted_masks\n","    valid_labels = torch.zeros((num_valid_users, num_games_filtered), device = device, dtype=torch.int)\n","    has_interacted_masks = torch.zeros((num_valid_users, num_games_filtered), device = device, dtype=torch.bool)\n","\n","    # Gán nhãn từ valid\n","    for user_id, interactions in user_valid.items():\n","        if user_id not in user_to_idx:\n","            continue\n","        u_idx = user_to_idx[user_id]\n","        for game_id, label in interactions:\n","            if game_id in game_to_idx:\n","                g_idx = game_to_idx[game_id]\n","                valid_labels[u_idx, g_idx] = label\n","\n","    # Gán mask từ train\n","    for user_id, played_games in user_played_games.items():\n","        if user_id not in user_to_idx:\n","            continue\n","        u_idx = user_to_idx[user_id]\n","        for game_id in played_games:\n","            if game_id in game_to_idx:\n","                g_idx = game_to_idx[game_id]\n","                has_interacted_masks[u_idx, g_idx] = True\n","\n","    # Lọc user có đủ số tương tác âm (min_neg)\n","    good_users = []\n","    for user_id in valid_user_ids:\n","        played = user_played_games.get(user_id, set())\n","        positives = {g for g, l in user_valid[user_id] if l == 1}\n","        unplayed = set(game_list) - played - positives\n","        if len(unplayed) >= min_neg:\n","            good_users.append(user_id)\n","\n","    # Cập nhật danh sách và tensors chỉ với good_users\n","    keep_indices = [user_to_idx[u] for u in good_users]\n","    valid_labels = valid_labels[keep_indices]\n","    has_interacted_masks = has_interacted_masks[keep_indices]\n","    valid_users_tensor = torch.tensor(good_users, device = device, dtype=torch.long)\n","\n","    # Mask cho warm items (game vừa có trong train vừa có trong valid)\n","    train_items = set(train_df[\"app_id\"])\n","    valid_items = set(valid_df[\"app_id\"])\n","    warm_items = train_items & valid_items\n","    warm_items_mask = torch.tensor([g in warm_items for g in game_list], device = device, dtype=torch.bool)\n","\n","    return FullCorpusData(\n","        labels = valid_labels,\n","        has_interacted_masks = has_interacted_masks,\n","        valid_users = valid_users_tensor,\n","        warm_item_mask = warm_items_mask\n","    )"]},{"cell_type":"code","source":["# Call the function with the full dataset and num_games\n","full_corpus_data = create_full_corpus_data(dataset, train_df, valid_df, num_games, device)"],"metadata":{"id":"LNITl_CJPnJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dG17UYWTZPyQ"},"source":["### **5.3.2. Khởi tạo mô hình và huấn luyện**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":172,"status":"ok","timestamp":1749838482405,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"},"user_tz":-420},"id":"8FgUXAl2BTBm","outputId":"a9fc5b92-a2b5-40f2-f565-a5c0e77232ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tham số của mô hình:\n","param_name: user_emb_gmf.weight, param_size: torch.Size([47274, 8]), requires_grad: True\n","param_name: item_emb_gmf.weight, param_size: torch.Size([4632, 8]), requires_grad: True\n","param_name: user_emb_mlp.weight, param_size: torch.Size([47274, 16]), requires_grad: True\n","param_name: item_emb_mlp.weight, param_size: torch.Size([4632, 16]), requires_grad: True\n","param_name: mlp.0.weight, param_size: torch.Size([16, 32]), requires_grad: True\n","param_name: mlp.0.bias, param_size: torch.Size([16]), requires_grad: True\n","param_name: mlp.3.weight, param_size: torch.Size([8, 16]), requires_grad: True\n","param_name: mlp.3.bias, param_size: torch.Size([8]), requires_grad: True\n","param_name: head.weight, param_size: torch.Size([1, 16]), requires_grad: True\n","param_name: head.bias, param_size: torch.Size([1]), requires_grad: True\n"]}],"source":["model = NeuMF(\n","    num_users = num_users,\n","    num_items = num_games,\n","    embedding_dim = 64,\n","    layers = [128, 64, 32],\n","    dropout = 0.4\n",").to(device)\n","\n","print(\"Tham số của mô hình:\")\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(f\"param_name: {name}, param_size: {param.size()}, requires_grad: {param.requires_grad}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKUbUeOJs9xF","outputId":"ce700f4f-199a-444c-a138-8e9f1454a8d0"},"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Train Loss: 0.4576 | Valid Loss: 0.8603 | NDCG@10: 0.0089 | HitRate@10: 0.0804\n","---> Best checkpoint is saved!\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 02 | Train Loss: 0.3032 | Valid Loss: 0.8716 | NDCG@10: 0.0094 | HitRate@10: 0.0668\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 03 | Train Loss: 0.2549 | Valid Loss: 0.9025 | NDCG@10: 0.0082 | HitRate@10: 0.0565\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 04 | Train Loss: 0.2368 | Valid Loss: 0.9037 | NDCG@10: 0.0078 | HitRate@10: 0.0551\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 05 | Train Loss: 0.2283 | Valid Loss: 0.9538 | NDCG@10: 0.0080 | HitRate@10: 0.0545\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 06 | Train Loss: 0.2234 | Valid Loss: 0.9959 | NDCG@10: 0.0080 | HitRate@10: 0.0543\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 07 | Train Loss: 0.2197 | Valid Loss: 1.0200 | NDCG@10: 0.0080 | HitRate@10: 0.0535\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 08 | Train Loss: 0.2170 | Valid Loss: 1.0578 | NDCG@10: 0.0072 | HitRate@10: 0.0499\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 09 | Train Loss: 0.2146 | Valid Loss: 1.0275 | NDCG@10: 0.0066 | HitRate@10: 0.0488\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 10 | Train Loss: 0.2126 | Valid Loss: 1.0782 | NDCG@10: 0.0067 | HitRate@10: 0.0481\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 11 | Train Loss: 0.2106 | Valid Loss: 1.0810 | NDCG@10: 0.0070 | HitRate@10: 0.0482\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 12 | Train Loss: 0.2094 | Valid Loss: 1.1094 | NDCG@10: 0.0076 | HitRate@10: 0.0498\n"]},{"output_type":"stream","name":"stderr","text":["Training:  46%|████▌     | 72/157 [00:11<00:11,  7.46it/s]"]}],"source":["optimizer = optim.Adam(model.parameters(), lr = 0.01)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 50, eta_min = 0.0005)\n","num_epochs = 50\n","\n","train_losses, valid_losses, ndcgs_10, hitrates_10 = [], [], [], []\n","best_ndcg_10 = 0.0\n","best_hitrate_10 = 0.0\n","\n","for epoch in range(1, num_epochs + 1):\n","   train_loss = train(\n","       model = model,\n","       train_data = train_data,\n","       train_loader = train_loader,\n","       optimizer = optimizer,\n","       scheduler = scheduler,\n","       device = device,\n","       lambda_reg = 0.001,\n","       criterion = 'bpr',\n","       num_neg = 3\n","   )\n","\n","   valid_loss = validate(\n","       model = model,\n","       valid_data = valid_data,\n","       valid_loader = valid_loader,\n","       device = device,\n","       criterion = 'bpr',\n","       num_neg = 3\n","   )\n","\n","   ndcg_10, hitrate_10 = evaluate_full_corpus(model, full_corpus_data, device, k = 10)\n","\n","   train_losses.append(train_loss)\n","   valid_losses.append(valid_loss)\n","   ndcgs_10.append(ndcg_10)\n","   hitrates_10.append(hitrate_10)\n","\n","   print(f\"Epoch: {epoch:02d} | Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f} | NDCG@10: {ndcg_10:.4f} | HitRate@10: {hitrate_10:.4f}\")\n","\n","   if ndcg_10 > best_ndcg_10 and hitrate_10 > best_hitrate_10:\n","      best_ndcg_10 = ndcg_10\n","      best_hitrate_10 = hitrate_10\n","      if os.path.exists(model_dir):\n","          os.remove(model_dir)\n","\n","      torch.save(model.state_dict(), model_dir)\n","      print(\"---> Best checkpoint is saved!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6n8iQAFA_dO"},"outputs":[],"source":["fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # 1 hàng, 3 cột\n","\n","# Create a range of epoch numbers for the x-axis\n","epochs_range = range(1, len(train_losses) + 1)\n","\n","# --- Plot 1: Training and Validation Losses ---\n","axes[0].plot(epochs_range, train_losses, label='Train Loss')\n","axes[0].plot(epochs_range, valid_losses, label='Valid Loss')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Loss')\n","axes[0].set_title('Train and Validation Loss')\n","axes[0].legend()\n","axes[0].grid(True)\n","\n","# --- Plot 2: NDCG@10 over Epochs ---\n","axes[1].plot(epochs_range, ndcgs_10, label='NDCG@10')\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Metric Value')\n","axes[1].set_title('NDCG@10 over Epochs')\n","axes[1].legend()\n","axes[1].grid(True)\n","\n","# --- Plot 3: HitRate@10 over Epochs ---\n","axes[2].plot(epochs_range, hitrates_10, label='HitRate@10')\n","axes[2].set_xlabel('Epoch')\n","axes[2].set_ylabel('Metric Value')\n","axes[2].set_title('HitRate@10 over Epochs')\n","axes[2].legend()\n","axes[2].grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7q9bLcBBjTPn"},"source":["# **6. Đánh giá khả năng xếp hạng (ranking)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71a59ruKDA4o"},"outputs":[],"source":["model = NeuMF(\n","    num_users = num_users,\n","    num_items = num_games,\n","    embedding_dim = 64,\n","    layers = [128, 64, 32],\n","    dropout = 0.2\n",").to(device)\n","\n","# Load model with proper device mapping\n","model.load_state_dict(torch.load(model_dir, map_location=device))\n","model.to(device)  # Move model to appropriate device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktc-M_gU7V89"},"outputs":[],"source":["def create_test_data(dataset_df, train_df, test_df, device, min_unplayed=9):\n","    # Create game list and mappings\n","    game_list = sorted(dataset_df[\"app_id\"].unique())\n","    game2idx = {game_id: idx for idx, game_id in enumerate(game_list)}\n","    num_games_filtered = len(game_list)\n","\n","    # Identify warm-start and cold-start users\n","    train_user_ids = set(train_df[\"user_id\"])\n","    test_user_ids = set(test_df[\"user_id\"])\n","    warm_user_ids = sorted(train_user_ids & test_user_ids)\n","    cold_user_ids = sorted(test_user_ids - train_user_ids)\n","    all_test_user_ids = warm_user_ids + cold_user_ids\n","    user2idx = {user_id: idx for idx, user_id in enumerate(all_test_user_ids)}\n","\n","    # Track interacted games for warm-start users (train)\n","    user_played_games = defaultdict(set)\n","    for user_id, game_id in zip(train_df[\"user_id\"], train_df[\"app_id\"]):\n","        if user_id in user2idx:\n","            user_played_games[user_id].add(game_id)\n","\n","    # Track test interactions for all users\n","    user_test = defaultdict(list)\n","    for user_id, game_id, label in zip(test_df[\"user_id\"], test_df[\"app_id\"], test_df[\"is_recommended\"]):\n","        user_test[user_id].append((game_id, label))\n","\n","    # Initialize tensors\n","    num_test_users = len(all_test_user_ids)\n","    test_labels = torch.zeros((num_test_users, num_games_filtered), dtype=torch.float, device=device)\n","    has_interacted_masks = torch.zeros((num_test_users, num_games_filtered), dtype=torch.bool, device=device)\n","\n","    # Fill tensors\n","    for user_id in all_test_user_ids:\n","        u_idx = user2idx[user_id]\n","        for game_id, label in user_test[user_id]:\n","            if game_id in game2idx:\n","                test_labels[u_idx, game2idx[game_id]] = float(label)\n","        for game_id in user_played_games.get(user_id, []):\n","            if game_id in game2idx:\n","                has_interacted_masks[u_idx, game2idx[game_id]] = True\n","\n","    # Filter users with too few unplayed games\n","    good_users = []\n","    for user_id in all_test_user_ids:\n","        played = user_played_games.get(user_id, set())\n","        positives = {g for g, l in user_test[user_id] if l == 1}\n","        unplayed = set(game_list) - played - positives\n","        if len(unplayed) >= min_unplayed:\n","            good_users.append(user_id)\n","\n","    keep_indices = [user2idx[u] for u in good_users]\n","    test_labels = test_labels[keep_indices]\n","    has_interacted_masks = has_interacted_masks[keep_indices]\n","    warm_users_mask = torch.tensor([u in train_user_ids for u in good_users],\n","                                   dtype=torch.bool, device=device)\n","    user_ids_tensor = torch.tensor(good_users, dtype=torch.long, device=device)\n","\n","    # Create mask for warm-start items\n","    train_items = set(train_df[\"app_id\"])\n","    warm_items_mask = torch.tensor([g in train_items for g in game_list],\n","                                   dtype=torch.bool, device=device)\n","\n","    return TestData(\n","        labels=test_labels,\n","        has_interacted_masks=has_interacted_masks,\n","        user_ids=user_ids_tensor,\n","        warm_items_mask=warm_items_mask,\n","        warm_users_mask=warm_users_mask\n","    )\n","\n","test_data = create_test_data(dataset, train_df, test_df, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-h5mzRNuR7Ev"},"outputs":[],"source":["@torch.no_grad()\n","def evaluate_ranking_full_corpus(model, test_data, device, k=10, batch_size=512, item_batch_size=500):\n","    print(\"Evaluating ranking metrics on full corpus...\")\n","    model.eval()\n","\n","    # Get test data\n","    test_labels = test_data.labels.to(device)\n","    has_interacted_masks = test_data.has_interacted_masks.to(device)\n","    user_ids = test_data.user_ids.to(device)\n","    warm_items_mask = test_data.warm_items_mask.to(device)\n","    warm_users_mask = test_data.warm_users_mask.to(device)\n","\n","    # Metrics storage\n","    all_prec = []\n","    all_recall = []\n","    all_ndcg = []\n","    all_hit = []\n","\n","    num_test = user_ids.size(0)\n","    num_items = model.num_items\n","\n","    # Process users in batches\n","    for start in tqdm(range(0, num_test, batch_size), desc=\"Evaluating\", leave=False):\n","        end = min(start + batch_size, num_test)\n","        batch_idx = slice(start, end)\n","        batch_user_ids = user_ids[batch_idx]\n","        batch_warm_mask = warm_users_mask[batch_idx]\n","\n","        # Initialize scores for all users with negative infinity\n","        batch_scores = torch.full((end-start, num_items), -float('inf'), device=device)\n","\n","        # Only process warm-start users\n","        if batch_warm_mask.any():\n","            warm_user_indices = torch.where(batch_warm_mask)[0]\n","            warm_user_ids = batch_user_ids[warm_user_indices]\n","\n","            # Process items in chunks for each warm user\n","            for item_start in range(0, num_items, item_batch_size):\n","                item_end = min(item_start + item_batch_size, num_items)\n","\n","                # Create user-item pairs for this chunk\n","                batch_users_expanded = warm_user_ids.unsqueeze(1).expand(-1, item_end - item_start)\n","                chunk_items = torch.arange(item_start, item_end, device=device).unsqueeze(0).expand(warm_user_ids.size(0), -1)\n","\n","                # Reshape for model forward pass\n","                batch_users_flat = batch_users_expanded.reshape(-1)\n","                chunk_items_flat = chunk_items.reshape(-1)\n","\n","                # Get scores for this chunk\n","                chunk_scores = model(batch_users_flat, chunk_items_flat)\n","\n","                # Reshape scores back to original dimensions\n","                chunk_scores = chunk_scores.reshape(warm_user_ids.size(0), item_end - item_start)\n","\n","                # Store scores in the appropriate positions\n","                for i, orig_idx in enumerate(warm_user_indices):\n","                    batch_scores[orig_idx, item_start:item_end] = chunk_scores[i]\n","\n","                # Clear memory\n","                del chunk_scores\n","                torch.cuda.empty_cache()\n","\n","        # Mask scores for already interacted items\n","        batch_scores = batch_scores.masked_fill(has_interacted_masks[batch_idx], -float('inf'))\n","        batch_labels = test_labels[batch_idx]\n","\n","        # Skip users with no positive labels\n","        valid_users = batch_labels.sum(dim=1) > 0\n","        if not valid_users.any():\n","            continue\n","\n","        batch_scores = batch_scores[valid_users]\n","        batch_labels = batch_labels[valid_users]\n","\n","        # Compute metrics\n","        prec = precision_at_k(batch_scores, batch_labels, k)\n","        recall = recall_at_k(batch_scores, batch_labels, k)\n","        ndcg = ndcg_at_k(batch_scores, batch_labels, k)\n","        hitrate = hitrate_at_k(batch_scores, batch_labels, k)\n","\n","        all_prec.append(prec)\n","        all_recall.append(recall)\n","        all_ndcg.append(ndcg)\n","        all_hit.append(hitrate)\n","\n","        # Clear memory\n","        del batch_scores\n","        torch.cuda.empty_cache()\n","\n","    # Compute average metrics\n","    all_prec = torch.cat(all_prec).mean().item() if all_prec else 0.0\n","    all_recall = torch.cat(all_recall).mean().item() if all_recall else 0.0\n","    all_ndcg = torch.cat(all_ndcg).mean().item() if all_ndcg else 0.0\n","    all_hit = torch.cat(all_hit).mean().item() if all_hit else 0.0\n","\n","    return {\n","        f\"Precision@{k}\": all_prec,\n","        f\"Recall@{k}\": all_recall,\n","        f\"NDCG@{k}\": all_ndcg,\n","        f\"HitRate@{k}\": all_hit,\n","    }"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}