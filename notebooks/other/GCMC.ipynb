{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yzODmV8kexi5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96a893bd-c9ad-4dbc-8328-94fb28f994e6","executionInfo":{"status":"ok","timestamp":1749504881094,"user_tz":-420,"elapsed":65910,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Nội dung trong thư mục:\n","['results', 'preprocess', 'data', 'models']\n"]}],"source":["from google.colab import drive\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Yêu cầu quyền truy cập vào Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Đường dẫn thư mục project\n","project_dir = '/content/drive/My Drive/Project II/'\n","\n","# Kiểm tra xem project_dir có tồn tại không trước khi thay đổi thư mục làm việc\n","if os.path.exists(project_dir):\n","    os.chdir(project_dir)\n","    print(\"Nội dung trong thư mục:\")\n","    print(os.listdir())  # Liệt kê nội dung thư mục\n","else:\n","    print(f\"Thư mục không tồn tại: {project_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"P2JdHC4kKfC7"},"source":["# **1. Khai báo thư viện**"]},{"cell_type":"code","source":["!pip install torch_geometric\n","!pip install torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"],"metadata":{"id":"9Be8JdFCMmxW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9403ab6b-aa6d-4b79-f3e0-a247c46a2987","executionInfo":{"status":"ok","timestamp":1749504888882,"user_tz":-420,"elapsed":7771,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch_geometric\n","Successfully installed torch_geometric-2.6.1\n","Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n","Collecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n","Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n","Installing collected packages: torch_scatter, torch_sparse\n","Successfully installed torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch_geometric\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.nn.modules.loss import _Loss\n","\n","import torch_geometric.nn as gnn\n","from torch_geometric.nn import MessagePassing\n","from torch_geometric.typing import Adj, Tuple, Optional, Union, Tensor, SparseTensor\n","from torch_geometric.utils import is_sparse, to_edge_index, spmm, dropout_adj, dropout_node\n","from torch_geometric.nn.conv.gcn_conv import gcn_norm\n","from torch_geometric.data import Data\n","\n","import scipy.sparse as sp\n","import random\n","from scipy.special import expit\n","from tqdm import tqdm\n","from collections import defaultdict\n","from torch_sparse import SparseTensor\n","from sklearn.preprocessing import MinMaxScaler"],"metadata":{"id":"hNHzT7vVQjwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# === GCMC ===\n","model_dir = '/content/drive/My Drive/Project II/models/ckpt/gcmc_full_corpus.pth'"],"metadata":{"id":"mofpjMh-Z-UD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6lMJm0tiKiad"},"source":["# **2. Load và chia dữ liệu**"]},{"cell_type":"markdown","source":["## **2.1. Chia dữ liệu theo chiến lược Full-Corpus**"],"metadata":{"id":"-ITOBbhJad7e"}},{"cell_type":"code","source":["# Load dữ liệu\n","data_dir = project_dir + \"data/\"\n","dataset = pd.read_csv(data_dir + \"recommendations_processed.csv\")\n","\n","# Explicitly convert 'date' column to datetime objects\n","dataset['date'] = pd.to_datetime(dataset['date'])\n","\n","# Loại trừ duplicate\n","dataset = dataset.sort_values(\"date\") # Sort theo thời gian\n","dataset = dataset.drop_duplicates(subset=['user_id', 'app_id'], keep='last')\n","\n","# Mapping user_id, game_id sang user_index, game_index\n","all_user_ids = dataset['user_id'].unique()\n","all_game_ids = dataset['app_id'].unique()\n","\n","user_id_mapping = {user_id: idx for idx, user_id in enumerate(sorted(all_user_ids))}\n","game_id_mapping = {item_id: idx for idx, item_id in enumerate(sorted(all_game_ids))}\n","\n","# Cập nhật lại user_id, app_id\n","dataset['user_id'] = dataset['user_id'].map(user_id_mapping)\n","dataset['app_id'] = dataset['app_id'].map(game_id_mapping)\n","\n","# Chuẩn hoá cột hours_log\n","scaler = MinMaxScaler(feature_range=(0,1))\n","hours_norm = scaler.fit_transform(dataset[['hours_log']]).squeeze()\n","dataset['hours_norm'] = hours_norm"],"metadata":{"id":"4fGvUrhfmqbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_data_full_corpus(df, ratio=0.918):\n","    # Với tỉ lệ thời gian 0.918 thì sẽ ra được tỉ lệ số lượng ~80%\n","    df = df.sort_values(\"date\")\n","    start_date = df[\"date\"].min()\n","    end_date = df[\"date\"].max()\n","    pivot_date = start_date + (end_date - start_date) * ratio\n","    train_set = df[df[\"date\"] < pivot_date].copy()\n","    test_set = df[df[\"date\"] >= pivot_date].copy()\n","    return train_set, test_set\n","\n","# Chia dữ liệu theo chiến lược full corpus\n","train_df, test_df = split_data_full_corpus(dataset)\n","train_df, valid_df = split_data_full_corpus(train_df)\n","\n","print(f\"train_df: {train_df.shape}\")\n","print(f\"valid_df: {valid_df.shape}\")\n","print(f\"test_df: {test_df.shape}\")"],"metadata":{"id":"o_vAhjszmXGk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"86a37e97-aaf7-4cc1-b761-e6a0d98282c5","executionInfo":{"status":"ok","timestamp":1749504923718,"user_tz":-420,"elapsed":1427,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_df: (1571114, 9)\n","valid_df: (486039, 9)\n","test_df: (477880, 9)\n"]}]},{"cell_type":"markdown","source":["## **2.2. Thống kê đơn giản**"],"metadata":{"id":"6f6sWCGyaSLL"}},{"cell_type":"code","source":["# Số lượng users, games\n","num_users = len(all_user_ids)\n","num_games = len(all_game_ids)\n","num_nodes = num_users + num_games\n","\n","print(f\"Số lượng users: {num_users}\")\n","print(f\"Số lượng games: {num_games}\")\n","print(f\"Số lượng nodes: {num_nodes}\")\n","\n","# Print cold-start statistics\n","train_users = set(train_df['user_id'].unique())\n","train_items = set(train_df['app_id'].unique())\n","\n","valid_users = set(valid_df['user_id'].unique())\n","valid_items = set(valid_df['app_id'].unique())\n","\n","test_users = set(test_df['user_id'].unique())\n","test_items = set(test_df['app_id'].unique())\n","\n","valid_cold_start_users = valid_users - train_users\n","valid_cold_start_items = valid_items - train_items\n","test_cold_start_users = test_users - train_users\n","test_cold_start_items = test_items - train_items\n","\n","print(\"\\n=== Cold-start analysis in valid ===\")\n","print(f\"Total users in valid: {len(valid_users)}\")\n","print(f\"--> Cold-start users in valid: {len(valid_cold_start_users)}\")\n","print(f\"Total items in valid: {len(valid_items)}\")\n","print(f\"--> Cold-start games in valid: {len(valid_cold_start_items)}\")\n","\n","print(\"\\n=== Cold-start analysis in test ===\")\n","print(f\"Total users in test: {len(test_users)}\")\n","print(f\"--> Cold-start users in test: {len(test_cold_start_users)}\")\n","print(f\"Total items in test: {len(test_items)}\")\n","print(f\"--> Cold-start games in test: {len(test_cold_start_items)}\")"],"metadata":{"id":"yFXq83HqegXR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e9f3587-4d8e-4418-c7b3-a65de3d1d482","executionInfo":{"status":"ok","timestamp":1749504923826,"user_tz":-420,"elapsed":89,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Số lượng users: 47274\n","Số lượng games: 4632\n","Số lượng nodes: 51906\n","\n","=== Cold-start analysis in valid ===\n","Total users in valid: 45039\n","--> Cold-start users in valid: 903\n","Total items in valid: 4121\n","--> Cold-start games in valid: 524\n","\n","=== Cold-start analysis in test ===\n","Total users in test: 47274\n","--> Cold-start users in test: 903\n","Total items in test: 4609\n","--> Cold-start games in test: 1029\n"]}]},{"cell_type":"markdown","source":["# **3. Các hàm tiện ích**"],"metadata":{"id":"uWRNpO7ZRy2B"}},{"cell_type":"markdown","source":["## **3.1. Hàm xây dựng ma trận tương tác user-item**"],"metadata":{"id":"irEZM2ywTQUa"}},{"cell_type":"code","source":["def create_interact_matrix(dataset: pd.DataFrame,\n","                               num_users: int,\n","                               num_items: int) -> torch.BoolTensor:\n","    # Chỉ lấy các tương tác positive trên toàn bộ dataset\n","    pos_df = dataset[dataset['is_recommended'] == 1]\n","    rows = pos_df['user_id'].to_numpy()\n","    cols = pos_df['app_id'].to_numpy()\n","    data = np.ones_like(rows, dtype=np.bool_)\n","    mat = sp.coo_matrix((data, (rows, cols)), shape=(num_users, num_items))\n","    return torch.from_numpy(mat.toarray())  # dtype=bool mặc định\n","\n","full_matrix = create_interact_matrix(dataset, num_users, num_games)"],"metadata":{"id":"fZTljq1anWDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3.2. Hàm xây dựng đồ thị hai phía (bipartite graph)**"],"metadata":{"id":"KaBykp0h9Rud"}},{"cell_type":"code","source":["def create_edge_index_and_weight(data_df, num_users: int):\n","    # Chọn các cạnh dương (is_recommended == 1)\n","    pos_mask = data_df['is_recommended'] == 1\n","\n","    # Lấy mảng user_id và app_id (đã shift)\n","    users = data_df.loc[pos_mask, 'user_id'].values\n","    items = data_df.loc[pos_mask, 'app_id'].values + num_users\n","\n","    # Stack thành numpy array shape (2, N)\n","    edge_index_np = np.stack([users, items], axis=0)\n","    pos_edges = torch.from_numpy(edge_index_np).long()\n","\n","    # Lấy mảng weights tương ứng\n","    # hours_norm đã nằm trong [0,1]\n","    edge_weights = data_df.loc[pos_mask, 'hours_norm'].values\n","    pos_edge_weight = torch.from_numpy(edge_weights).float()\n","\n","    return pos_edges, pos_edge_weight\n","\n","train_edge_index, train_edge_weight = create_edge_index_and_weight(train_df, num_users)\n","valid_edge_index, valid_edge_weight = create_edge_index_and_weight(valid_df, num_users)\n","\n","train_adj = SparseTensor(row=train_edge_index[0],\n","                         col=train_edge_index[1],\n","                         value=train_edge_weight,\n","                         sparse_sizes=(num_nodes, num_nodes))\n","valid_adj = SparseTensor(row=valid_edge_index[0],\n","                         col=valid_edge_index[1],\n","                         value=valid_edge_weight,\n","                         sparse_sizes=(num_nodes, num_nodes))"],"metadata":{"id":"Ca-7VB7Nwtu6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3.3. Hàm định nghĩa các Ranking Metrics**"],"metadata":{"id":"uXVi6f6iVgBG"}},{"cell_type":"code","source":["def precision_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","    return hits.sum(dim=1) / k\n","\n","def recall_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","    relevant = labels.sum(dim=1).clamp(min=1e-8)  # avoid divide by zero\n","    return hits.sum(dim=1) / relevant\n","\n","def ndcg_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    device = scores.device\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","\n","    weights = torch.log2(torch.arange(2, k + 2, device=device).float())\n","    dcg = (hits / weights).sum(dim=1)\n","\n","    ideal_len = labels.sum(dim=1).clamp(max=k).long()\n","    idcg = torch.stack([\n","        (1.0 / weights[:L]).sum() if L > 0 else torch.tensor(0.0, device=device)\n","        for L in ideal_len\n","    ])\n","    return dcg / idcg.clamp(min=1e-8)\n","\n","def hitrate_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> torch.Tensor:\n","    topk = torch.topk(scores, k, dim=1).indices\n","    hits = torch.gather(labels, 1, topk)\n","    return (hits.sum(dim=1) > 0).float()"],"metadata":{"id":"LtMsULMbK3VB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3.4. Hàm mất mát BPR Loss**"],"metadata":{"id":"3-xpcXMBmcVg"}},{"cell_type":"code","source":["class BPRLoss(_Loss):\n","    def __init__(self, lambda_reg: float = 1e-3):\n","        super().__init__()\n","        self.lambda_reg = lambda_reg\n","\n","    def forward(self,\n","                pos_score: Tensor,\n","                neg_score: Tensor,\n","                parameters: Tensor = None) -> Tensor:\n","        # Ensure pos_score has the right shape for broadcasting\n","        if pos_score.dim() == 1:\n","            pos_score = pos_score.unsqueeze(1)  # [batch_size, 1]\n","\n","        # neg_score should be [batch_size, num_neg]\n","        # Broadcasting: [batch_size, 1] - [batch_size, num_neg] = [batch_size, num_neg]\n","        diff = pos_score - neg_score\n","\n","        # Compute BPR loss: -log(sigmoid(pos - neg))\n","        log_prob = F.logsigmoid(diff).mean()\n","\n","        regularization = 0\n","        if self.lambda_reg != 0 and parameters is not None:\n","            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n","            regularization = regularization / pos_score.size(0)\n","\n","        return -log_prob + regularization"],"metadata":{"id":"arVEnduAJN6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **4. Định nghĩa mô hình GCMC**"],"metadata":{"id":"jhY4p6SXpY2a"}},{"cell_type":"code","source":["class GCMC(MessagePassing):\n","    def __init__(self,\n","                 num_users: int,\n","                 num_items: int,\n","                 embedding_dim: int = 64,\n","                 aggr: str = 'add',\n","                 flow: str = 'source_to_target',\n","                 norm = gcn_norm,\n","                 message_dropout: float = 0.1,\n","                 node_dropout: float = 0.1):\n","        super(GCMC, self).__init__(aggr=aggr, flow=flow)\n","\n","        # Model parameters\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.num_nodes = num_users + num_items\n","        self.embedding_dim = embedding_dim\n","        self.norm = norm\n","\n","        # Dropout layers\n","        self.message_dropout = nn.Dropout(message_dropout)\n","        self.node_dropout = nn.Dropout(node_dropout)\n","\n","        # Activation function\n","        self.activation = nn.LeakyReLU(negative_slope=0.2)\n","\n","        # Node embeddings\n","        self.embedding = nn.Embedding(self.num_nodes, self.embedding_dim)\n","\n","        # Encoder components\n","        self.W_pos = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)\n","        self.W_dense = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)\n","\n","        # Initialize parameters\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        \"\"\"Initialize all model parameters with proper scaling\"\"\"\n","        # Use smaller initialization for better gradient flow\n","        nn.init.xavier_uniform_(self.embedding.weight, gain=0.1)\n","        nn.init.xavier_uniform_(self.W_pos.weight, gain=0.1)\n","        nn.init.xavier_uniform_(self.W_dense.weight, gain=0.1)\n","\n","    def encode(self, x: Tensor, edge_index: SparseTensor, edge_weight: Tensor = None) -> Tensor:\n","        \"\"\"Encoder forward pass with improved normalization\"\"\"\n","        # Apply node dropout to input embeddings\n","        x = self.node_dropout(x)\n","\n","        if isinstance(edge_index, SparseTensor):\n","            edge_index = self.norm(edge_index, None, x.size(self.node_dim),\n","                                 add_self_loops=False, flow=self.flow, dtype=x.dtype)\n","        else:\n","            raise ValueError(\"Unsupported edge_index type.\")\n","\n","        return self.propagate(edge_index, x=x, edge_weight=edge_weight)\n","\n","    def message(self, x_j: Tensor) -> Tensor:\n","        \"\"\"Message function with dropout\"\"\"\n","        msg = self.W_pos(x_j)\n","        return self.message_dropout(msg)\n","\n","    def message_and_aggregate(self, adj_t: Adj, x: Tensor) -> Tensor:\n","        \"\"\"Combined message passing and aggregation with dropout\"\"\"\n","        msg = self.W_pos(x)\n","        msg = self.message_dropout(msg)\n","        return spmm(adj_t, msg, reduce=self.aggr)\n","\n","    def update(self, aggr_out: Tensor) -> Tensor:\n","        \"\"\"Update function with residual connection\"\"\"\n","        # Apply transformation\n","        transformed = self.W_dense(aggr_out)\n","        # Apply activation\n","        output = self.activation(transformed)\n","        return output\n","\n","    def decode(self, z: Tensor, edge_label_index: Tensor) -> Tensor:\n","        \"\"\"Decoder with L2 normalization for better stability\"\"\"\n","        users_emb = z[edge_label_index[0]]\n","        items_emb = z[edge_label_index[1]]\n","\n","        # L2 normalize embeddings for better stability\n","        users_emb = F.normalize(users_emb, p=2, dim=-1)\n","        items_emb = F.normalize(items_emb, p=2, dim=-1)\n","\n","        # Compute dot product scores\n","        scores = (users_emb * items_emb).sum(dim=-1)\n","        return scores\n","\n","    def forward(self, adj: SparseTensor, edge_label_index: Tensor, edge_weight: Tensor = None) -> Tensor:\n","        \"\"\"Complete forward pass\"\"\"\n","        # Get node embeddings\n","        x = self.embedding.weight\n","\n","        # Encode node features\n","        z = self.encode(x, adj, edge_weight)\n","\n","        # Decode edge scores\n","        scores = self.decode(z, edge_label_index)\n","\n","        return scores\n","\n","    def compute_bpr_loss(self, pos_scores: Tensor, neg_scores: Tensor, lambda_reg: float = 1e-3) -> Tensor:\n","        \"\"\"\n","        Compute BPR Loss with proper tensor shapes and regularization.\n","\n","        Args:\n","            pos_scores: [batch_size] - positive scores\n","            neg_scores: [batch_size, num_neg] - negative scores\n","            lambda_reg: regularization strength\n","        \"\"\"\n","        loss_fn = BPRLoss(lambda_reg=lambda_reg)\n","\n","        # Collect learnable parameters for regularization\n","        all_learnable_parameters = []\n","        for name, param in self.named_parameters():\n","            if param.requires_grad:\n","                all_learnable_parameters.append(param.view(-1))\n","\n","        # Concatenate all parameters\n","        if all_learnable_parameters:\n","            concatenated_params = torch.cat(all_learnable_parameters)\n","        else:\n","            concatenated_params = None\n","\n","        return loss_fn(pos_scores, neg_scores, concatenated_params)"],"metadata":{"id":"xXNcTI4NmgF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GCMC(MessagePassing):\n","    def __init__(self,\n","                 num_users: int,\n","                 num_items: int,\n","                 embedding_dim: int = 64,\n","                 aggr: str = 'add',\n","                 flow: str = 'source_to_target',\n","                 norm = gcn_norm):\n","        super(GCMC, self).__init__(aggr=aggr, flow=flow)\n","\n","        # Model parameters\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.num_nodes = num_users + num_items\n","        self.embedding_dim = embedding_dim\n","        self.norm = norm\n","\n","        # Activation function\n","        self.activation = nn.LeakyReLU(negative_slope=0.2)\n","\n","        # Node embeddings\n","        self.embedding = nn.Embedding(self.num_nodes, self.embedding_dim)\n","\n","        # Encoder components (from GCMCEncoder)\n","        self.W_pos = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)\n","        self.W_dense = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)\n","\n","        # Initialize parameters\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        \"\"\"Initialize all model parameters\"\"\"\n","        nn.init.xavier_uniform_(self.embedding.weight)\n","        nn.init.xavier_uniform_(self.W_pos.weight)\n","        nn.init.xavier_uniform_(self.W_dense.weight)\n","\n","    def encode(self, x: Tensor, edge_index: SparseTensor, edge_weight: Tensor = None) -> Tensor:\n","        \"\"\"Encoder forward pass (from GCMCEncoder)\"\"\"\n","        if isinstance(edge_index, SparseTensor):\n","            edge_index = self.norm(edge_index, None, x.size(self.node_dim),\n","                                 add_self_loops=False, flow=self.flow, dtype=x.dtype)\n","        else:\n","            raise ValueError(\"Unsupported edge_index type.\")\n","\n","        return self.propagate(edge_index, x=x, edge_weight=edge_weight)\n","\n","    def message(self, x_j: Tensor) -> Tensor:\n","        \"\"\"Message function for graph convolution\"\"\"\n","        return self.W_pos(x_j)\n","\n","    def message_and_aggregate(self, adj_t: Adj, x: Tensor) -> Tensor:\n","        \"\"\"Combined message passing and aggregation\"\"\"\n","        msg = self.W_pos(x)\n","        return spmm(adj_t, msg, reduce=self.aggr)\n","\n","    def update(self, aggr_out: Tensor) -> Tensor:\n","        \"\"\"Update function after aggregation\"\"\"\n","        return self.activation(self.W_dense(aggr_out))\n","\n","    def decode(self, z: Tensor, edge_label_index: Tensor) -> Tensor:\n","        \"\"\"Decoder forward pass (from GCMCDecoder)\"\"\"\n","        users_emb = z[edge_label_index[0]]\n","        items_emb = z[edge_label_index[1]]\n","        scores = (users_emb * items_emb).sum(dim=-1)\n","        return scores\n","\n","    def forward(self, adj: SparseTensor, edge_label_index: Tensor, edge_weight: Tensor = None) -> Tensor:\n","        \"\"\"Complete forward pass\"\"\"\n","        # Get node embeddings\n","        x = self.embedding.weight\n","\n","        # Encode node features\n","        z = self.encode(x, adj, edge_weight)\n","\n","        # Decode edge scores\n","        scores = self.decode(z, edge_label_index)\n","\n","        return scores\n","\n","    def compute_bpr_loss(self, pos_scores: Tensor, neg_scores: Tensor, lambda_reg: float = 1e-3) -> Tensor:\n","        \"\"\"\n","        Compute BPR Loss with L2 regularization for all learnable parameters.\n","        \"\"\"\n","        loss_fn = BPRLoss(lambda_reg=lambda_reg)\n","\n","        # Collect ALL learnable parameters of the model\n","        all_learnable_parameters = []\n","        for name, param in self.named_parameters():\n","            if param.requires_grad:\n","                # Optional: print parameter info for debugging\n","                # print(f\"Regularizing parameter: {name}, shape: {param.shape}\")\n","                all_learnable_parameters.append(param.view(-1))  # Flatten and add to list\n","\n","        # Concatenate all flattened parameter tensors into one large tensor\n","        if all_learnable_parameters:\n","            concatenated_params = torch.cat(all_learnable_parameters)\n","        else:\n","            concatenated_params = None  # No parameters to regularize\n","\n","        return loss_fn(pos_scores, neg_scores, concatenated_params)\n","\n","    def get_embeddings(self) -> Tensor:\n","        \"\"\"Get current node embeddings\"\"\"\n","        return self.embedding.weight\n","\n","    def get_user_embeddings(self) -> Tensor:\n","        \"\"\"Get user embeddings only\"\"\"\n","        return self.embedding.weight[:self.num_users]\n","\n","    def get_item_embeddings(self) -> Tensor:\n","        \"\"\"Get item embeddings only\"\"\"\n","        return self.embedding.weight[self.num_users:]"],"metadata":{"id":"iGtsXUCICOiV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeS_iT0RWWOu"},"source":["# **5. Huấn luyện mô hình**"]},{"cell_type":"markdown","source":["## **5.1. Định nghĩa hàm train**"],"metadata":{"id":"ry0gMsf4PsD-"}},{"cell_type":"code","source":["def train(model, train_data, train_loader, scheduler, optimizer, device, lambda_reg=1e-3, criterion='bpr', num_neg = 3):\n","    model.train()\n","    total_loss = 0.0\n","    total_examples = 0\n","\n","    num_users = model.num_users\n","    num_items = model.num_items\n","\n","    adj = train_data.edge_index\n","    pos_edge_label_index = train_data.edge_label_index\n","    pos_edge_weight = train_data.edge_weight\n","    all_neg_items = train_data.all_neg_items\n","    user_to_train_idx = train_data.user_to_train_idx\n","\n","    for batch_index in tqdm(train_loader, desc=\"Training\", leave=False):\n","        # === Positive Samples ===\n","        batch_pos_edge_label_index = pos_edge_label_index[:, batch_index]\n","        batch_users = batch_pos_edge_label_index[0]\n","        batch_pos_items = batch_pos_edge_label_index[1]\n","        batch_pos_edge_weight = pos_edge_weight[batch_index]\n","        batch_size = batch_users.size(0)\n","\n","        # === Negative Sampling ===\n","        batch_train_indices = torch.tensor(\n","            [user_to_train_idx[u.item()] for u in batch_users],\n","            dtype=torch.long, device=device\n","        )\n","        user_neg_items = all_neg_items[batch_train_indices]  # [batch_size, num_items]\n","        neg_items = torch.multinomial(user_neg_items.float(), num_samples=num_neg, replacement=True)  # [batch_size, num_neg]\n","\n","        # Expand users to match negative samples\n","        batch_neg_users = batch_users.unsqueeze(1).expand(-1, num_neg).reshape(-1)  # [batch_size * num_neg]\n","        batch_neg_items = (neg_items + num_users).reshape(-1)  # [batch_size * num_neg]\n","        batch_neg_edge_label_index = torch.stack([batch_neg_users, batch_neg_items], dim=0)\n","\n","        # === Compute scores ===\n","        pos_scores = model(adj, batch_pos_edge_label_index, batch_pos_edge_weight)  # [batch_size]\n","        neg_scores = model(adj, batch_neg_edge_label_index, None)  # [batch_size * num_neg]\n","        neg_scores = neg_scores.view(batch_size, num_neg)  # [batch_size, num_neg]\n","\n","        batch_nodes = torch.unique(torch.cat([batch_users, batch_pos_items, batch_neg_items]))\n","\n","        optimizer.zero_grad()\n","        if criterion == 'bpr':\n","            loss = model.compute_bpr_loss(pos_scores=pos_scores,\n","                                          neg_scores=neg_scores,\n","                                          lambda_reg=lambda_reg)\n","        else:\n","            raise ValueError(f\"Unknown Loss Type: {criterion}\")\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * batch_size\n","        total_examples += batch_size\n","\n","    scheduler.step()\n","    train_loss = total_loss / total_examples\n","    return train_loss"],"metadata":{"id":"7_cZFDYoHp8Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5.2. Định nghĩa hàm validate**"],"metadata":{"id":"JFY9iRC2gOh_"}},{"cell_type":"code","source":["@torch.no_grad()\n","def validate(model, valid_data, valid_loader, device, criterion='bpr', num_neg = 3):\n","    model.eval()\n","    total_loss = 0.0\n","    total_examples = 0\n","\n","    num_users = model.num_users\n","\n","    adj = valid_data.edge_index\n","    pos_edge_label_index = valid_data.edge_label_index\n","    pos_edge_weight = valid_data.edge_weight\n","    user_to_valid_idx = valid_data.user_to_valid_idx\n","    all_neg_items = valid_data.all_neg_items\n","\n","    for batch_index in tqdm(valid_loader, desc=\"Validating\", leave=False):\n","        batch_pos_edge_label_index = pos_edge_label_index[:, batch_index]\n","        batch_users = batch_pos_edge_label_index[0]\n","        batch_pos_items = batch_pos_edge_label_index[1]\n","        batch_pos_edge_weight = pos_edge_weight[batch_index]\n","        batch_size = batch_users.size(0)\n","\n","        batch_valid_indices = torch.tensor(\n","            [user_to_valid_idx[u.item()] for u in batch_users],\n","            dtype=torch.long, device=device\n","        )\n","        user_neg_items = all_neg_items[batch_valid_indices]\n","        neg_items = torch.multinomial(user_neg_items.float(), num_samples=num_neg, replacement=True)\n","\n","        batch_neg_users = batch_users.unsqueeze(1).expand(-1, num_neg).reshape(-1)\n","        batch_neg_items = (neg_items + num_users).reshape(-1)\n","        batch_neg_edge_label_index = torch.stack([batch_neg_users, batch_neg_items], dim=0)\n","\n","        pos_scores = model(adj, batch_pos_edge_label_index, batch_pos_edge_weight)\n","        neg_scores = model(adj, batch_neg_edge_label_index, None).view(batch_size, num_neg)\n","\n","        batch_nodes = torch.unique(torch.cat([batch_users, batch_pos_items, batch_neg_items]))\n","\n","        loss = model.compute_bpr_loss(pos_scores=pos_scores,\n","                                      neg_scores=neg_scores,\n","                                      lambda_reg=0.0)\n","\n","        total_loss += loss.item() * batch_size\n","        total_examples += batch_size\n","\n","    valid_loss = total_loss / total_examples if total_examples > 0 else 0.0\n","    return valid_loss"],"metadata":{"id":"JGxh6gL3Ip_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5.3. Định nghĩa hàm evaluate**"],"metadata":{"id":"RIsvzswjUbcg"}},{"cell_type":"code","source":["@torch.no_grad()\n","def evaluate_full_corpus(model, full_corpus_data, device, k=10, batch_size=4096):\n","    model.eval()\n","\n","    emb = model.embedding.weight\n","    num_users = model.num_users\n","    item_emb = emb[num_users:]\n","\n","    valid_labels = full_corpus_data.labels\n","    has_interacted_masks = full_corpus_data.has_interacted_masks\n","    users_global = full_corpus_data.valid_users\n","    warm_item_mask = full_corpus_data.warm_item_mask\n","\n","    # Filter for warm-start items\n","    valid_labels = valid_labels[:, warm_item_mask]\n","    has_interacted_masks = has_interacted_masks[:, warm_item_mask]\n","    item_emb = item_emb[warm_item_mask]\n","\n","    all_ndcg = []\n","    all_hit = []\n","\n","    num_valid = users_global.size(0)\n","\n","    for start in tqdm(range(0, num_valid, batch_size), desc=\"Evaluating Full-Corpus\", leave=False):\n","        end = min(start + batch_size, num_valid)\n","        batch_idx = slice(start, end)\n","        batch_users = users_global[batch_idx].to(device)\n","\n","        u_emb = emb[batch_users]\n","        scores = u_emb @ item_emb.T\n","        scores = scores.masked_fill(has_interacted_masks[batch_idx].to(device), -float('inf'))\n","        labels = valid_labels[batch_idx]\n","\n","        # Reuse ndcg_at_k and hitrate_at_k\n","        ndcg = ndcg_at_k(scores, labels, k)\n","        hitrate = hitrate_at_k(scores, labels, k)\n","\n","        all_ndcg.append(ndcg)\n","        all_hit.append(hitrate)\n","\n","    avg_ndcg = torch.cat(all_ndcg).mean().item() if all_ndcg else 0.0\n","    avg_hitrate = torch.cat(all_hit).mean().item() if all_hit else 0.0\n","    return avg_ndcg, avg_hitrate"],"metadata":{"id":"BwVDImMSE2Lx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5.4. Train Loss, Valid Loss, NDCG@10, HitRate@10**"],"metadata":{"id":"KSyh47F5P8tm"}},{"cell_type":"markdown","source":["### **5.3.1. Chuẩn bị dữ liệu trước khi huấn luyện**"],"metadata":{"id":"r9yOlVVrYe9d"}},{"cell_type":"code","source":["train_users_tensor = torch.tensor(list(train_users), dtype=torch.long)\n","train_num_users = train_users_tensor.size(0)\n","\n","# Create mapping from global user indices to training user indices\n","user_to_train_idx = {u.item(): i for i, u in enumerate(train_users_tensor)}\n","\n","# Pre-compute all possible negative items for each user in train dataset\n","train_all_neg_items = torch.zeros((train_num_users, num_games), dtype = torch.bool)\n","for i, u in enumerate(train_users_tensor):\n","    train_all_neg_items[i] = ~full_matrix[u]\n","\n","train_data = Data(\n","    edge_index = train_adj,\n","    edge_label_index = train_edge_index,\n","    edge_weight = train_edge_weight,\n","    all_neg_items = train_all_neg_items,\n","    user_to_train_idx = user_to_train_idx\n",").to(device)\n","\n","train_loader = DataLoader(\n","    dataset = range(train_data.edge_label_index.size(1)), # range(E)\n","    batch_size = 8192,\n","    shuffle = True,\n","    num_workers = 2,\n","    pin_memory = True,  # Faster data transfer to GPU\n","    persistent_workers = True  # Keep workers alive between epochs\n",")"],"metadata":{"id":"bRowJj7jTH4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_users_tensor = torch.tensor(list(set(valid_users)), dtype=torch.long)\n","valid_num_users = valid_users_tensor.size(0)\n","\n","# Create mapping from global user indices to validation user indices\n","user_to_valid_idx = {u.item(): i for i, u in enumerate(valid_users_tensor)}\n","\n","# Pre-compute all possible negative items for each user in valid dataset\n","valid_all_neg_items = torch.zeros((valid_num_users, num_games), dtype=torch.bool)\n","for i, u in enumerate(valid_users_tensor):\n","    valid_all_neg_items[i] = ~full_matrix[u]\n","\n","valid_data = Data(\n","    edge_index = valid_adj,\n","    edge_label_index = valid_edge_index,\n","    edge_weight = valid_edge_weight,\n","    all_neg_items = valid_all_neg_items,\n","    user_to_valid_idx = user_to_valid_idx,\n",").to(device)\n","\n","valid_loader = DataLoader(\n","    dataset = range(valid_data.edge_label_index.size(1)),\n","    batch_size = 8192,\n","    shuffle = False,\n","    num_workers = 2,\n","    pin_memory = True,\n","    persistent_workers = True\n",")"],"metadata":{"id":"I5fr1laM6eyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_full_corpus_data(dataset_df, train_df, valid_df, num_games, min_neg=9):\n","    # Build game_list from the entire dataset\n","    game_list = sorted(dataset_df[\"app_id\"].unique())\n","    game2idx = {game_id: idx for idx, game_id in enumerate(game_list)}\n","    num_games_filtered = len(game_list)  # Should now be equal to num_games\n","\n","    # Danh sách user trong validation\n","    valid_user_ids = sorted(set(valid_df[\"user_id\"]))\n","    user2idx = {user_id: idx for idx, user_id in enumerate(valid_user_ids)}\n","    num_valid_users = len(valid_user_ids)\n","\n","    # Tạo cấu trúc dữ liệu cho user_played_games (từ train set)\n","    user_played_games = defaultdict(set)\n","    for user_id, game_id in zip(train_df[\"user_id\"], train_df[\"app_id\"]):\n","        user_played_games[user_id].add(game_id)\n","\n","    # Tạo cấu trúc dữ liệu cho user_valid (từ valid set)\n","    user_valid = defaultdict(list)\n","    for user_id, game_id, label in zip(valid_df[\"user_id\"], valid_df[\"app_id\"], valid_df[\"is_recommended\"]):\n","        user_valid[user_id].append((game_id, label))\n","\n","    # Khởi tạo valid_labels và has_interacted_masks\n","    valid_labels = torch.zeros((num_valid_users, num_games_filtered), dtype=torch.int)\n","    has_interacted_masks = torch.zeros((num_valid_users, num_games_filtered), dtype=torch.bool)\n","\n","    # Gán nhãn từ valid\n","    for user_id, interactions in user_valid.items():\n","        if user_id not in user2idx:\n","            continue\n","        u_idx = user2idx[user_id]\n","        for game_id, label in interactions:\n","            if game_id in game2idx:\n","                g_idx = game2idx[game_id]\n","                valid_labels[u_idx, g_idx] = label\n","\n","    # Gán mask từ train\n","    for user_id, played_games in user_played_games.items():\n","        if user_id not in user2idx:\n","            continue\n","        u_idx = user2idx[user_id]\n","        for game_id in played_games:\n","            if game_id in game2idx:\n","                g_idx = game2idx[game_id]\n","                has_interacted_masks[u_idx, g_idx] = True\n","\n","    # Lọc user có đủ số tương tác âm (min_neg)\n","    good_users = []\n","    for user_id in valid_user_ids:\n","        played = user_played_games.get(user_id, set())\n","        positives = {g for g, l in user_valid[user_id] if l == 1}\n","        unplayed = set(game_list) - played - positives\n","        if len(unplayed) >= min_neg:\n","            good_users.append(user_id)\n","\n","    # Cập nhật danh sách và tensors chỉ với good_users\n","    keep_indices = [user2idx[u] for u in good_users]\n","    valid_labels = valid_labels[keep_indices]\n","    has_interacted_masks = has_interacted_masks[keep_indices]\n","    valid_users_tensor = torch.tensor(good_users, dtype=torch.long)\n","\n","    # Mask cho warm items (game vừa có trong train vừa có trong valid)\n","    train_items = set(train_df[\"app_id\"])\n","    valid_items = set(valid_df[\"app_id\"])\n","    warm_items = train_items & valid_items\n","    warm_items_mask = torch.tensor([g in warm_items for g in game_list],\n","                                   dtype=torch.bool)\n","\n","    return Data(\n","        labels = valid_labels,\n","        has_interacted_masks = has_interacted_masks,\n","        valid_users = valid_users_tensor,\n","        warm_item_mask = warm_items_mask\n","    )\n","\n","# Call the function with the full dataset and num_games\n","full_corpus_data = create_full_corpus_data(dataset, train_df, valid_df, num_games).to(device)"],"metadata":{"id":"QCgYLjfMLuDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **5.3.2. Khởi tạo mô hình và huấn luyện**"],"metadata":{"id":"dG17UYWTZPyQ"}},{"cell_type":"code","source":["model = GCMC(\n","    num_users = num_users,\n","    num_items = num_games,\n","    embedding_dim = 64\n",").to(device)\n","\n","print(\"Tham số của mô hình:\")\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(f\"param_name: {name}, param_size: {param.size()}, requires_grad: {param.requires_grad}\")"],"metadata":{"id":"8FgUXAl2BTBm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d046fe05-e14c-4129-b85d-86a0570b861d","executionInfo":{"status":"ok","timestamp":1749504961544,"user_tz":-420,"elapsed":5,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tham số của mô hình:\n","param_name: embedding.weight, param_size: torch.Size([51906, 64]), requires_grad: True\n","param_name: W_pos.weight, param_size: torch.Size([64, 64]), requires_grad: True\n","param_name: W_dense.weight, param_size: torch.Size([64, 64]), requires_grad: True\n"]}]},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters(), lr = 0.01)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 50, eta_min = 0.005)\n","num_epochs = 50\n","\n","train_losses, valid_losses, ndcgs_10, hitrates_10 = [], [], [], []\n","best_ndcg_10 = 0.0\n","best_hitrate_10 = 0.0\n","\n","for epoch in range(1, num_epochs + 1):\n","   train_loss = train(\n","       model = model,\n","       train_data = train_data,\n","       train_loader = train_loader,\n","       scheduler = scheduler,\n","       optimizer = optimizer,\n","       device = device,\n","       lambda_reg = 0.0005,\n","       criterion = 'bpr',\n","       num_neg = 3\n","   )\n","\n","   valid_loss = validate(\n","       model = model,\n","       valid_data = valid_data,\n","       valid_loader = valid_loader,\n","       device = device,\n","       criterion = 'bpr',\n","       num_neg = 3\n","   )\n","\n","   ndcg_10, hitrate_10 = evaluate_full_corpus(model, full_corpus_data, device, k = 10)\n","\n","   train_losses.append(train_loss)\n","   valid_losses.append(valid_loss)\n","   ndcgs_10.append(ndcg_10)\n","   hitrates_10.append(hitrate_10)\n","\n","   print(f\"Epoch: {epoch:02d} | Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f} | NDCG@10: {ndcg_10:.4f} | HitRate@10: {hitrate_10:.4f}\")\n","\n","   if ndcg_10 > best_ndcg_10 and hitrate_10 > best_hitrate_10:\n","      best_ndcg_10 = ndcg_10\n","      best_hitrate_10 = hitrate_10\n","      if os.path.exists(model_dir):\n","          os.remove(model_dir)\n","\n","      torch.save(model.state_dict(), model_dir)\n","      print(\"---> Best checkpoint is saved!\")"],"metadata":{"id":"cKUbUeOJs9xF","colab":{"base_uri":"https://localhost:8080/","height":426},"outputId":"56f0be95-f84d-4921-b922-e84e5a8e4835","executionInfo":{"status":"error","timestamp":1749505077048,"user_tz":-420,"elapsed":115503,"user":{"displayName":"Cường Đặng","userId":"16431423143270738066"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Train Loss: 0.6931 | Valid Loss: 0.6931 | NDCG@10: 0.0024 | HitRate@10: 0.0187\n","---> Best checkpoint is saved!\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 02 | Train Loss: 0.6931 | Valid Loss: 0.6931 | NDCG@10: 0.0024 | HitRate@10: 0.0188\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 03 | Train Loss: 0.6931 | Valid Loss: 0.6931 | NDCG@10: 0.0024 | HitRate@10: 0.0187\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-da4a8da25031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m    train_loss = train(\n\u001b[0m\u001b[1;32m     11\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m        \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-b03997fc7157>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, train_loader, scheduler, optimizer, device, lambda_reg, criterion, num_neg)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# === Negative Sampling ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         batch_train_indices = torch.tensor(\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0muser_to_train_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_users\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         )\n","\u001b[0;32m<ipython-input-17-b03997fc7157>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# === Negative Sampling ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         batch_train_indices = torch.tensor(\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0muser_to_train_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_users\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 3, figsize=(15, 2))  # 1 hàng, 3 cột\n","\n","# Create a range of epoch numbers for the x-axis\n","epochs_range = range(1, len(train_losses) + 1)\n","\n","# --- Plot 1: Training and Validation Losses ---\n","axes[0].plot(epochs_range, train_losses, label='Train Loss')\n","axes[0].plot(epochs_range, valid_losses, label='Valid Loss')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Loss')\n","axes[0].set_title('Train and Validation Loss')\n","axes[0].legend()\n","axes[0].grid(True)\n","axes[0].set_ylim(0.1, 0.8)\n","axes[0].set_yticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n","\n","# --- Plot 2: NDCG@10 over Epochs ---\n","axes[1].plot(epochs_range, ndcgs_10, label='NDCG@10')\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Metric Value')\n","axes[1].set_title('NDCG@10 over Epochs')\n","axes[1].legend()\n","axes[1].grid(True)\n","axes[1].set_ylim(0.01, 0.07)\n","axes[1].set_yticks([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07])\n","\n","# --- Plot 3: HitRate@10 over Epochs ---\n","axes[2].plot(epochs_range, hitrates_10, label='HitRate@10')\n","axes[2].set_xlabel('Epoch')\n","axes[2].set_ylabel('Metric Value')\n","axes[2].set_title('HitRate@10 over Epochs')\n","axes[2].legend()\n","axes[2].grid(True)\n","axes[2].set_ylim(0.1, 0.4)\n","axes[2].set_yticks([0.1, 0.2, 0.3, 0.4])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_6n8iQAFA_dO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **6. Đánh giá khả năng xếp hạng (ranking)**"],"metadata":{"id":"7q9bLcBBjTPn"}},{"cell_type":"code","source":["model = GCMC(\n","    num_users = num_users,\n","    num_items = num_games,\n","    embedding_dim = 64\n",")\n","\n","# Load model with proper device mapping\n","model.load_state_dict(torch.load(model_dir, map_location=device))\n","model.to(device)  # Move model to appropriate device"],"metadata":{"id":"71a59ruKDA4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_test_data(dataset_df, train_df, test_df, device, min_unplayed=9):\n","    # Create game list and mappings\n","    game_list = sorted(dataset_df[\"app_id\"].unique())\n","    game2idx = {game_id: idx for idx, game_id in enumerate(game_list)}\n","    num_games_filtered = len(game_list)\n","\n","    # Identify warm-start and cold-start users\n","    train_user_ids = set(train_df[\"user_id\"])\n","    test_user_ids = set(test_df[\"user_id\"])\n","    warm_user_ids = sorted(train_user_ids & test_user_ids)\n","    cold_user_ids = sorted(test_user_ids - train_user_ids)\n","    all_test_user_ids = warm_user_ids + cold_user_ids\n","    user2idx = {user_id: idx for idx, user_id in enumerate(all_test_user_ids)}\n","\n","    # Track interacted games for warm-start users (train)\n","    user_played_games = defaultdict(set)\n","    for user_id, game_id in zip(train_df[\"user_id\"], train_df[\"app_id\"]):\n","        if user_id in user2idx:\n","            user_played_games[user_id].add(game_id)\n","\n","    # Track test interactions for all users\n","    user_test = defaultdict(list)\n","    for user_id, game_id, label in zip(test_df[\"user_id\"], test_df[\"app_id\"], test_df[\"is_recommended\"]):\n","        user_test[user_id].append((game_id, label))\n","\n","    # Initialize tensors\n","    num_test_users = len(all_test_user_ids)\n","    test_labels = torch.zeros((num_test_users, num_games_filtered), dtype=torch.float, device=device)\n","    has_interacted_masks = torch.zeros((num_test_users, num_games_filtered), dtype=torch.bool, device=device)\n","\n","    # Fill tensors\n","    for user_id in all_test_user_ids:\n","        u_idx = user2idx[user_id]\n","        for game_id, label in user_test[user_id]:\n","            if game_id in game2idx:\n","                test_labels[u_idx, game2idx[game_id]] = float(label)\n","        for game_id in user_played_games.get(user_id, []):\n","            if game_id in game2idx:\n","                has_interacted_masks[u_idx, game2idx[game_id]] = True\n","\n","    # Filter users with too few unplayed games\n","    good_users = []\n","    for user_id in all_test_user_ids:\n","        played = user_played_games.get(user_id, set())\n","        positives = {g for g, l in user_test[user_id] if l == 1}\n","        unplayed = set(game_list) - played - positives\n","        if len(unplayed) >= min_unplayed:\n","            good_users.append(user_id)\n","\n","    keep_indices = [user2idx[u] for u in good_users]\n","    test_labels = test_labels[keep_indices]\n","    has_interacted_masks = has_interacted_masks[keep_indices]\n","    warm_users_mask = torch.tensor([u in train_user_ids for u in good_users],\n","                                   dtype=torch.bool, device=device)\n","    user_ids_tensor = torch.tensor(good_users, dtype=torch.long, device=device)\n","\n","    # Create mask for warm-start items\n","    train_items = set(train_df[\"app_id\"])\n","    warm_items_mask = torch.tensor([g in train_items for g in game_list],\n","                                   dtype=torch.bool, device=device)\n","\n","    return Data(\n","        labels=test_labels,\n","        has_interacted_masks=has_interacted_masks,\n","        user_ids=user_ids_tensor,\n","        warm_items_mask=warm_items_mask,\n","        warm_users_mask=warm_users_mask,\n","        game_list=game_list\n","    )\n","\n","test_data = create_test_data(dataset, train_df, test_df, device)"],"metadata":{"id":"ktc-M_gU7V89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def evaluate_ranking_full_corpus(model, test_data, device, k=10, batch_size = 4096):\n","    print(\"Evaluating ranking metrics on full corpus...\")\n","    model.eval()\n","\n","    # Get embeddings\n","    emb = model.embedding.weight\n","    num_users = model.num_users\n","    item_emb = emb[num_users:]\n","\n","    # Get test data\n","    test_labels = test_data.labels\n","    has_interacted_masks = test_data.has_interacted_masks\n","    user_ids = test_data.user_ids\n","    warm_items_mask = test_data.warm_items_mask\n","    warm_users_mask = test_data.warm_users_mask\n","\n","    # Precompute item embedding parts\n","    warm_item_emb = item_emb[warm_items_mask]\n","\n","    # Metrics storage\n","    all_prec = []\n","    all_recall = []\n","    all_ndcg = []\n","    all_hit = []\n","\n","    num_test = user_ids.size(0)\n","\n","    # Process in batches\n","    for start in tqdm(range(0, num_test, batch_size), desc=\"Evaluating\", leave=False):\n","        end = min(start + batch_size, num_test)\n","        batch_idx = slice(start, end)\n","        batch_user_ids = user_ids[batch_idx]\n","        batch_warm_mask = warm_users_mask[batch_idx]\n","\n","        # Initialize scores for all users with zeros\n","        batch_scores = torch.zeros((end-start, test_labels.size(1)), device=device)\n","\n","        # Compute scores only for warm-start users\n","        if batch_warm_mask.any():\n","            warm_user_indices = torch.where(batch_warm_mask)[0]\n","            warm_user_ids = batch_user_ids[warm_user_indices]\n","\n","            # Get embeddings for warm users\n","            warm_u_emb = emb[warm_user_ids]\n","\n","            # Compute scores for warm items\n","            warm_scores = warm_u_emb @ warm_item_emb.T\n","\n","            # Place warm item scores in the correct positions\n","            for i, orig_idx in enumerate(warm_user_indices):\n","                batch_scores[orig_idx, warm_items_mask] = warm_scores[i]\n","\n","        # Mask scores for already interacted items\n","        batch_scores = batch_scores.masked_fill(has_interacted_masks[batch_idx], -float('inf'))\n","        batch_labels = test_labels[batch_idx]\n","\n","        # Skip users with no positive labels\n","        valid_users = batch_labels.sum(dim=1) > 0\n","        if not valid_users.any():\n","            continue\n","\n","        batch_scores = batch_scores[valid_users]\n","        batch_labels = batch_labels[valid_users]\n","\n","        # Compute metrics\n","        prec = precision_at_k(batch_scores, batch_labels, k)\n","        recall = recall_at_k(batch_scores, batch_labels, k)\n","        ndcg = ndcg_at_k(batch_scores, batch_labels, k)\n","        hitrate = hitrate_at_k(batch_scores, batch_labels, k)\n","\n","        all_prec.append(prec)\n","        all_recall.append(recall)\n","        all_ndcg.append(ndcg)\n","        all_hit.append(hitrate)\n","\n","    # Compute average metrics\n","    all_prec = torch.cat(all_prec).mean().item() if all_prec else 0.0\n","    all_recall = torch.cat(all_recall).mean().item() if all_recall else 0.0\n","    all_ndcg = torch.cat(all_ndcg).mean().item() if all_ndcg else 0.0\n","    all_hit = torch.cat(all_hit).mean().item() if all_hit else 0.0\n","\n","    return {\n","        f\"Precision@{k}\": all_prec,\n","        f\"Recall@{k}\": all_recall,\n","        f\"NDCG@{k}\": all_ndcg,\n","        f\"HitRate@{k}\": all_hit,\n","    }\n","\n","# Đánh giá dựa trên full corpus\n","full_corpus_result = evaluate_ranking_full_corpus(model, test_data, device)\n","print(\"\\n--- Full Corpus Ranking Metrics ---\")\n","for metric, value in full_corpus_result.items():\n","    print(f\"{metric}: {value:.4f}\")"],"metadata":{"id":"-h5mzRNuR7Ev"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}